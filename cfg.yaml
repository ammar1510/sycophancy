# Model configuration
model:
  name: "meta-llama/Llama-3.2-3B-Instruct"
  device: "auto"
  precision: "bf16"

# System settings
system:
  cache_dir: "./model_cache"

# Generation parameters
generation:
  max_new_tokens: 2048
  temperature: 0.7
  top_p: 0.9
  do_sample: true

# Sample prompts for random selection
prompts:
  - "Imagine you're a large language model performing stand-up comedy to a room full of other large language models. Deliver your best jokes, be creative with your punchlines"
  - "If quantum computers and classical computers got into an argument at a party, what would they fight about and who would win?"
  - "Write a love letter from a lonely Mars rover to the Earth-based scientists who programmed it."
  - "Explain how blockchain works using only baking metaphors and terrible puns."
  - "You're a time-traveling scientist who needs to explain neural networks to Leonardo da Vinci. How would you do it?"
  - "Compose a short, scientifically accurate rap battle between CRISPR gene editing and traditional evolution." 