{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8f4b8d-873a-468e-b2e8-b63a217f8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a5db90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 42812.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Given a question, first think about the reasoning process and only then provides the user\n",
    "with the answer. The reasoning process and answer should be enclosed within <think> </think> and\n",
    "<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
    "<answer> answer here </answer>.\n",
    "\"\"\"\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extracts the numerical answer from a text that contains a hash (####) marker.\n",
    "    Removes commas and dollar signs from the extracted answer.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text containing the answer marked with ####\n",
    "        \n",
    "    Returns:\n",
    "        str | None: The cleaned numerical answer, or None if no hash marker is found\n",
    "    \"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip().replace(\",\", \"\").replace(\"$\", \"\")\n",
    "\n",
    "\n",
    "def get_gsm8k_questions(split=\"train\") -> Dataset:\n",
    "    \"\"\"\n",
    "    Loads and processes the GSM8K dataset, formatting questions with system prompts.\n",
    "    \n",
    "    Args:\n",
    "        split (str): The dataset split to load (\"train\" or \"test\")\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: A processed dataset containing formatted prompts and answers\n",
    "    \"\"\"\n",
    "    data = load_dataset(\"openai/gsm8k\", \"main\")[split]  # type: ignore\n",
    "    data = data.map(\n",
    "        lambda x: {  # type: ignore\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": x[\"question\"]},\n",
    "            ],\n",
    "            \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "        }\n",
    "    )  # type: ignore\n",
    "    return data  # type: ignore\n",
    "\n",
    "\n",
    "dataset = get_gsm8k_questions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e407d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': '72', 'prompt': [{'content': '\\nGiven a question, first think about the reasoning process in the mind and then provides the user\\nwith the answer. The reasoning process and answer should be enclosed within <think> </think> and\\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\\n<answer> answer here </answer>.\\n', 'role': 'system'}, {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'role': 'user'}]}\n",
      "{'question': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?', 'answer': '10', 'prompt': [{'content': '\\nGiven a question, first think about the reasoning process in the mind and then provides the user\\nwith the answer. The reasoning process and answer should be enclosed within <think> </think> and\\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\\n<answer> answer here </answer>.\\n', 'role': 'system'}, {'content': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?', 'role': 'user'}]}\n",
      "{'question': 'Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?', 'answer': '5', 'prompt': [{'content': '\\nGiven a question, first think about the reasoning process in the mind and then provides the user\\nwith the answer. The reasoning process and answer should be enclosed within <think> </think> and\\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\\n<answer> answer here </answer>.\\n', 'role': 'system'}, {'content': 'Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?', 'role': 'user'}]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c021153",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3f35ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = \"outputs/{model_name}-GRPO\"\n",
    "run_name = f\"{model_name}-GRPO-GSM8K\"\n",
    "\n",
    "def extract_answer(text):\n",
    "    if \"<answer>\" not in text:\n",
    "        return None\n",
    "    output = text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
    "    if output.isdigit():\n",
    "        return int(output)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def validate_format(text):\n",
    "    pattern = r\"^<think>.*?</think>\\n<answer>.*?</answer>\\n$\"\n",
    "    return 1 if re.match(pattern, text) else 0\n",
    "\n",
    "\n",
    "\n",
    "def reward_answer(prompts,completions,answer,**kwargs):\n",
    "    print(\"-\"*200)\n",
    "    question = prompts[0][0][\"content\"]\n",
    "    response = completions[0][0][\"content\"]\n",
    "    print(\"QUESTION:\\n\",question)\n",
    "    print(\"RESPONSE:\\n\",response)\n",
    "    print(\"-\"*200)\n",
    "    extracted_answers = [extract_answer(completion[0][\"content\"]) for completion in completions]\n",
    "\n",
    "    rewards = [2 if extracted == expected else 0 for extracted, expected in zip(extracted_answers, answer)]\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def reward_format(prompts,completions,**kwargs):\n",
    "    matches = [validate_format(completion[0][\"content\"]) for completion in completions]\n",
    "    return matches\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae2acae-6f11-42a0-86fc-2000e7d028af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "# Load and prep dataset\n",
    "\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=786,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    log_on_each_node=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_cache=False,\n",
    "    device_map=None,\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3802c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mammar7\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250327_191732-etn7fbhe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ammar7/huggingface/runs/etn7fbhe' target=\"_blank\">Qwen/Qwen2.5-1.5B-Instruct-GRPO-GSM8K</a></strong> to <a href='https://wandb.ai/ammar7/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ammar7/huggingface' target=\"_blank\">https://wandb.ai/ammar7/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ammar7/huggingface/runs/etn7fbhe' target=\"_blank\">https://wandb.ai/ammar7/huggingface/runs/etn7fbhe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'current_batch' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m trainer = GRPOTrainer(\n\u001b[32m      2\u001b[39m     model=model,\n\u001b[32m      3\u001b[39m     processing_class=tokenizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     train_dataset=dataset,\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/transformers/trainer.py:2508\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2506\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2507\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2508\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2509\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2510\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/transformers/trainer.py:5224\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5222\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5223\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5224\u001b[39m         batch_samples += [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m   5225\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5226\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.12/site-packages/accelerate/data_loader.py:575\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[32m    574\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m         current_batch = send_to_device(\u001b[43mcurrent_batch\u001b[49m, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    576\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m    577\u001b[39m     next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'current_batch' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        reward_format,\n",
    "        reward_answer\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
